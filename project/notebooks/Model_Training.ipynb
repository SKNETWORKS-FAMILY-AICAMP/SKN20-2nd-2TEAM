{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a251163e",
   "metadata": {},
   "source": [
    "# Gym Churn Prediction - ëª¨ë¸ í•™ìŠµ ë° íŠœë‹\n",
    "\n",
    "## í”„ë¡œì íŠ¸ ê°œìš”\n",
    "- **ë°ì´í„°ì…‹**: gym_churn_us.csv\n",
    "- **ëª©í‘œ**: í—¬ìŠ¤ì¥ íšŒì›ì˜ ì´íƒˆ(Churn) ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ\n",
    "- **ë°©ë²•**: ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹, ì•™ìƒë¸”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a69aaf",
   "metadata": {},
   "source": [
    "## 1ï¸. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b14341cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix, \n",
    "                             classification_report, roc_curve)\n",
    "\n",
    "# ML ëª¨ë¸\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ë”¥ëŸ¬ë‹\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# í†µê³„\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f54981",
   "metadata": {},
   "source": [
    "## 2ï¸. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00eaac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "ë°ì´í„° í¬ê¸°: (4000, 14)\n",
      "ê²°ì¸¡ì¹˜: 0ê°œ\n",
      "ì „ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: (4000, 14)\n",
      "ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\n",
      "íŠ¹ì„± ìˆ˜: 13\n",
      "ìƒ˜í”Œ ìˆ˜: 4,000\n",
      "ì´íƒˆë¥ : 26.52%\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "print(\"ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "data = pd.read_csv('../data/raw/gym_churn_us.csv')\n",
    "\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {data.shape}\")\n",
    "print(f\"ê²°ì¸¡ì¹˜: {data.isnull().sum().sum()}ê°œ\")\n",
    "\n",
    "# NaN ì œê±°\n",
    "data_clean = data.dropna()\n",
    "print(f\"ì „ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {data_clean.shape}\")\n",
    "\n",
    "# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "X = data_clean.drop('Churn', axis=1)\n",
    "y = data_clean['Churn']\n",
    "\n",
    "print(f\"ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"íŠ¹ì„± ìˆ˜: {X.shape[1]}\")\n",
    "print(f\"ìƒ˜í”Œ ìˆ˜: {X.shape[0]:,}\")\n",
    "print(f\"ì´íƒˆë¥ : {y.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c988c4",
   "metadata": {},
   "source": [
    "## 3ï¸. Train-Test ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "673e41c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test ë¶„í• \n",
      "Train í¬ê¸°: (3200, 13)\n",
      "Test í¬ê¸°: (800, 13)\n",
      "ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# Train-Test ë¶„í• \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train-Test ë¶„í• \")\n",
    "print(f\"Train í¬ê¸°: {X_train.shape}\")\n",
    "print(f\"Test í¬ê¸°: {X_test.shape}\")\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687d101d",
   "metadata": {},
   "source": [
    "## 4ï¸. SMOTE ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa95283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE ì ìš© ì¤‘...\n",
      "SMOTE ì ìš© ì „: (3200, 13)\n",
      "SMOTE ì ìš© í›„: (4702, 13)\n",
      "\n",
      "í´ë˜ìŠ¤ ë¶„í¬ (ì ìš© ì „): {0: 2351, 1: 849}\n",
      "í´ë˜ìŠ¤ ë¶„í¬ (ì ìš© í›„): {0: 2351, 1: 2351}\n",
      " SMOTE ì ìš© ì™„ë£Œ! í´ë˜ìŠ¤ê°€ ê· í˜•ì„ ì´ë£¹ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# SMOTE ì ìš©\n",
    "print(\"SMOTE ì ìš© ì¤‘...\")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"SMOTE ì ìš© ì „: {X_train_scaled.shape}\")\n",
    "print(f\"SMOTE ì ìš© í›„: {X_train_smote.shape}\")\n",
    "print(f\"\\ní´ë˜ìŠ¤ ë¶„í¬ (ì ìš© ì „): {y_train.value_counts().to_dict()}\")\n",
    "print(f\"í´ë˜ìŠ¤ ë¶„í¬ (ì ìš© í›„): {pd.Series(y_train_smote).value_counts().to_dict()}\")\n",
    "print(\" SMOTE ì ìš© ì™„ë£Œ! í´ë˜ìŠ¤ê°€ ê· í˜•ì„ ì´ë£¹ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d52874",
   "metadata": {},
   "source": [
    "## 5ï¸. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45955e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘...\n",
      "================================================================================\n",
      "ì›ë³¸ íŠ¹ì„± ìˆ˜: 13\n",
      "í–¥ìƒëœ íŠ¹ì„± ìˆ˜: 24\n",
      "ì¶”ê°€ëœ íŠ¹ì„±: 11ê°œ\n",
      " íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ!\n",
      "ìµœì¢… Train í¬ê¸°: (4702, 24)\n"
     ]
    }
   ],
   "source": [
    "# íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ - ì¤‘ìš” íŠ¹ì„± ê°„ ìƒí˜¸ì‘ìš©\n",
    "print(\"ğŸ”§ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ì›ë³¸ ë°ì´í„°ì— ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±\n",
    "X_enhanced = X.copy()\n",
    "\n",
    "# 1. Lifetime ê¸°ë°˜ íŒŒìƒ íŠ¹ì„±\n",
    "X_enhanced['Lifetime_per_Month'] = X_enhanced['Lifetime'] / (X_enhanced['Contract_period'] + 1)\n",
    "X_enhanced['Is_New_Member'] = (X_enhanced['Lifetime'] <= 2).astype(int)\n",
    "X_enhanced['Is_Long_Member'] = (X_enhanced['Lifetime'] >= 12).astype(int)\n",
    "\n",
    "# 2. ìˆ˜ì—… ì°¸ì—¬ìœ¨ ê´€ë ¨\n",
    "X_enhanced['Class_Engagement'] = X_enhanced['Avg_class_frequency_total'] * X_enhanced['Lifetime']\n",
    "X_enhanced['Recent_Activity'] = X_enhanced['Avg_class_frequency_current_month'] / (X_enhanced['Avg_class_frequency_total'] + 0.001)\n",
    "\n",
    "# 3. ê³„ì•½ ê´€ë ¨\n",
    "X_enhanced['Contract_Completion'] = 1 - (X_enhanced['Month_to_end_contract'] / (X_enhanced['Contract_period'] + 1))\n",
    "X_enhanced['Long_Contract'] = (X_enhanced['Contract_period'] >= 12).astype(int)\n",
    "\n",
    "# 4. ë¹„ìš© ê´€ë ¨\n",
    "X_enhanced['Cost_per_Visit'] = X_enhanced['Avg_additional_charges_total'] / (X_enhanced['Avg_class_frequency_total'] + 1)\n",
    "X_enhanced['High_Spender'] = (X_enhanced['Avg_additional_charges_total'] > X_enhanced['Avg_additional_charges_total'].median()).astype(int)\n",
    "\n",
    "# 5. ì°¸ì—¬ë„ ì§€í‘œ\n",
    "X_enhanced['Engagement_Score'] = (\n",
    "    X_enhanced['Group_visits'] + \n",
    "    X_enhanced['Partner'] + \n",
    "    X_enhanced['Promo_friends']\n",
    ")\n",
    "\n",
    "# 6. ë¦¬ìŠ¤í¬ ì§€í‘œ\n",
    "X_enhanced['Churn_Risk'] = (\n",
    "    (X_enhanced['Lifetime'] <= 3).astype(int) * 2 +\n",
    "    (X_enhanced['Avg_class_frequency_current_month'] < 1).astype(int) +\n",
    "    (X_enhanced['Month_to_end_contract'] <= 1).astype(int)\n",
    ")\n",
    "\n",
    "print(f\"ì›ë³¸ íŠ¹ì„± ìˆ˜: {X.shape[1]}\")\n",
    "print(f\"í–¥ìƒëœ íŠ¹ì„± ìˆ˜: {X_enhanced.shape[1]}\")\n",
    "print(f\"ì¶”ê°€ëœ íŠ¹ì„±: {X_enhanced.shape[1] - X.shape[1]}ê°œ\")\n",
    "\n",
    "# ìƒˆë¡œìš´ ë°ì´í„°ë¡œ Train-Test ë¶„í• \n",
    "X_train_enh, X_test_enh, y_train_enh, y_test_enh = train_test_split(\n",
    "    X_enhanced, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§\n",
    "scaler_enh = StandardScaler()\n",
    "X_train_enh_scaled = scaler_enh.fit_transform(X_train_enh)\n",
    "X_test_enh_scaled = scaler_enh.transform(X_test_enh)\n",
    "\n",
    "# SMOTE ì ìš©\n",
    "smote_enh = SMOTE(random_state=42)\n",
    "X_train_enh_smote, y_train_enh_smote = smote_enh.fit_resample(X_train_enh_scaled, y_train_enh)\n",
    "\n",
    "print(f\" íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ!\")\n",
    "print(f\"ìµœì¢… Train í¬ê¸°: {X_train_enh_smote.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58990503",
   "metadata": {},
   "source": [
    "## 6ï¸. ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56061d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n",
      " Logistic Regression í•™ìŠµ ì¤‘...\n",
      "  F1 Score: 0.8630\n",
      "  AUC: 0.9770\n",
      " Decision Tree í•™ìŠµ ì¤‘...\n",
      "  F1 Score: 0.8109\n",
      "  AUC: 0.8781\n",
      " Random Forest í•™ìŠµ ì¤‘...\n",
      "  F1 Score: 0.8389\n",
      "  AUC: 0.9670\n",
      " Gradient Boosting í•™ìŠµ ì¤‘...\n",
      "  F1 Score: 0.8941\n",
      "  AUC: 0.9770\n",
      " XGBoost í•™ìŠµ ì¤‘...\n",
      "  F1 Score: 0.8847\n",
      "  AUC: 0.9785\n",
      " LightGBM í•™ìŠµ ì¤‘...\n",
      "  F1 Score: 0.8825\n",
      "  AUC: 0.9797\n",
      "ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n",
      " Baseline ëª¨ë¸ ì„±ëŠ¥ (F1 Score ê¸°ì¤€ ì •ë ¬)\n",
      "                     accuracy  precision  recall      f1     auc\n",
      "Gradient Boosting      0.9438     0.8920  0.8962  0.8941  0.9770\n",
      "XGBoost                0.9388     0.8826  0.8868  0.8847  0.9785\n",
      "LightGBM               0.9388     0.8976  0.8679  0.8825  0.9797\n",
      "Logistic Regression    0.9250     0.8363  0.8915  0.8630  0.9770\n",
      "Random Forest          0.9150     0.8429  0.8349  0.8389  0.9670\n",
      "Decision Tree          0.8962     0.7841  0.8396  0.8109  0.8781\n"
     ]
    }
   ],
   "source": [
    "# 6ê°œ ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ\n",
    "print(\"ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜\n",
    "baseline_models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=150, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=150, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=150, random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "baseline_results = {}\n",
    "\n",
    "# ê° ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\" {name} í•™ìŠµ ì¤‘...\")\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # í‰ê°€\n",
    "    baseline_results[name] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    print(f\"  F1 Score: {baseline_results[name]['f1']:.4f}\")\n",
    "    print(f\"  AUC: {baseline_results[name]['auc']:.4f}\")\n",
    "\n",
    "print(\"ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "# ê²°ê³¼ DataFrame\n",
    "baseline_df = pd.DataFrame(baseline_results).T.sort_values('f1', ascending=False)\n",
    "print(\" Baseline ëª¨ë¸ ì„±ëŠ¥ (F1 Score ê¸°ì¤€ ì •ë ¬)\")\n",
    "print(baseline_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a990c",
   "metadata": {},
   "source": [
    "## 7ï¸. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e6bd7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘\n",
      " XGBoost íŠœë‹ ì™„ë£Œ!\n",
      "ìµœì  íŒŒë¼ë¯¸í„°: {'colsample_bytree': 0.9141362604455774, 'gamma': 0.3344941273571143, 'learning_rate': 0.12613732428729094, 'max_depth': 13, 'min_child_weight': 3, 'n_estimators': 300, 'subsample': 0.6020246335384875}\n"
     ]
    }
   ],
   "source": [
    "# XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "print(\"XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘\")\n",
    "\n",
    "# ìµœì  íŒŒë¼ë¯¸í„° (ì´ë¯¸ íƒìƒ‰ëœ ê²°ê³¼ ì‚¬ìš©)\n",
    "xgb_best_params = {\n",
    "    'colsample_bytree': 0.9141362604455774,\n",
    "    'gamma': 0.3344941273571143,\n",
    "    'learning_rate': 0.12613732428729094,\n",
    "    'max_depth': 13,\n",
    "    'min_child_weight': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.6020246335384875\n",
    "}\n",
    "\n",
    "xgb_tuned = XGBClassifier(**xgb_best_params, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_tuned.fit(X_train_enh_smote, y_train_enh_smote)\n",
    "\n",
    "print(\" XGBoost íŠœë‹ ì™„ë£Œ!\")\n",
    "print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {xgb_best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f4276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘...\n",
      "LightGBM íŠœë‹ ì™„ë£Œ!\n",
      "ìµœì  íŒŒë¼ë¯¸í„°: {'colsample_bytree': 0.871025744736913, 'learning_rate': 0.013317565785571231, 'max_depth': 7, 'min_child_samples': 28, 'n_estimators': 700, 'num_leaves': 71, 'subsample': 0.9762093057958416}\n"
     ]
    }
   ],
   "source": [
    "# LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "print(\" LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘...\")\n",
    "\n",
    "# ìµœì  íŒŒë¼ë¯¸í„° (ì´ë¯¸ íƒìƒ‰ëœ ê²°ê³¼ ì‚¬ìš©)\n",
    "lgb_best_params = {\n",
    "    'colsample_bytree': 0.871025744736913,\n",
    "    'learning_rate': 0.013317565785571231,\n",
    "    'max_depth': 7,\n",
    "    'min_child_samples': 28,\n",
    "    'n_estimators': 700,\n",
    "    'num_leaves': 71,\n",
    "    'subsample': 0.9762093057958416\n",
    "}\n",
    "\n",
    "lgb_tuned = LGBMClassifier(**lgb_best_params, random_state=42, verbose=-1)\n",
    "lgb_tuned.fit(X_train_enh_smote, y_train_enh_smote)\n",
    "\n",
    "print(\"LightGBM íŠœë‹ ì™„ë£Œ!\")\n",
    "print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {lgb_best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc1b9d",
   "metadata": {},
   "source": [
    "## 8ï¸. ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edbb2149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step\n",
      " ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n",
      "  F1 Score: 0.8879\n",
      "  AUC: 0.9809\n"
     ]
    }
   ],
   "source": [
    "# ë”¥ëŸ¬ë‹ ëª¨ë¸: Advanced Neural Network\n",
    "print(\"ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„±\n",
    "nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_enh_smote.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ì½œë°± ì„¤ì •\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# í•™ìŠµ\n",
    "history_nn = nn_model.fit(\n",
    "    X_train_enh_smote, y_train_enh_smote,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "y_pred_nn = (nn_model.predict(X_test_enh_scaled) > 0.5).astype(int).flatten()\n",
    "y_pred_proba_nn = nn_model.predict(X_test_enh_scaled).flatten()\n",
    "\n",
    "# í‰ê°€\n",
    "nn_results = {\n",
    "    'accuracy': accuracy_score(y_test_enh, y_pred_nn),\n",
    "    'precision': precision_score(y_test_enh, y_pred_nn),\n",
    "    'recall': recall_score(y_test_enh, y_pred_nn),\n",
    "    'f1': f1_score(y_test_enh, y_pred_nn),\n",
    "    'auc': roc_auc_score(y_test_enh, y_pred_proba_nn)\n",
    "}\n",
    "\n",
    "print(f\" ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"  F1 Score: {nn_results['f1']:.4f}\")\n",
    "print(f\"  AUC: {nn_results['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1dfec",
   "metadata": {},
   "source": [
    "## 9ï¸. ì•™ìƒë¸” ëª¨ë¸ (Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74a52ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶• ì¤‘...\n",
      "í•™ìŠµ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n",
      "ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# Stacking Ensemble\n",
    "print(\" ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶• ì¤‘...\")\n",
    "\n",
    "# ìµœì í™”ëœ Base ëª¨ë¸ë“¤\n",
    "estimators_ultimate = [\n",
    "    ('xgb', xgb_tuned),\n",
    "    ('lgb', lgb_tuned),\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=500, \n",
    "        max_depth=30, \n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )),\n",
    "    ('gb', GradientBoostingClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=7,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    ))\n",
    "]\n",
    "\n",
    "# Ultimate Stacking\n",
    "stacking_ultimate = StackingClassifier(\n",
    "    estimators=estimators_ultimate,\n",
    "    final_estimator=LogisticRegression(max_iter=2000, C=0.1, class_weight='balanced'),\n",
    "    cv=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"í•™ìŠµ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "stacking_ultimate.fit(X_train_enh_smote, y_train_enh_smote)\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "y_pred_proba_ultimate = stacking_ultimate.predict_proba(X_test_enh_scaled)[:, 1]\n",
    "\n",
    "print(\"ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf4318",
   "metadata": {},
   "source": [
    "## 10. ì„ê³„ê°’ ìµœì í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "067c5772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì  ì„ê³„ê°’ íƒìƒ‰ ì¤‘...\n",
      " ìµœì  ì„ê³„ê°’: 0.3000\n",
      "ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ (í–¥ìƒëœ ë°ì´í„° + ìµœì í™”)\n",
      "  Accuracy:  0.9563\n",
      "  Precision: 0.9041\n",
      "  Recall:    0.9340\n",
      "  F1 Score:  0.9188\n",
      "  AUC:       0.9851\n",
      "F1 Score 0.9 ë‹¬ì„±!\n"
     ]
    }
   ],
   "source": [
    "# ìµœì  ì„ê³„ê°’ íƒìƒ‰ (ë” ì„¸ë°€í•˜ê²Œ)\n",
    "print(\"ìµœì  ì„ê³„ê°’ íƒìƒ‰ ì¤‘...\")\n",
    "best_f1_ultimate = 0\n",
    "best_threshold_ultimate = 0.5\n",
    "best_results_ultimate = {}\n",
    "\n",
    "for threshold in np.arange(0.1, 0.9, 0.005):  # ë” ì„¸ë°€í•œ íƒìƒ‰\n",
    "    y_pred_temp = (y_pred_proba_ultimate >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test_enh, y_pred_temp)\n",
    "    \n",
    "    if f1 > best_f1_ultimate:\n",
    "        best_f1_ultimate = f1\n",
    "        best_threshold_ultimate = threshold\n",
    "        best_results_ultimate = {\n",
    "            'accuracy': accuracy_score(y_test_enh, y_pred_temp),\n",
    "            'precision': precision_score(y_test_enh, y_pred_temp),\n",
    "            'recall': recall_score(y_test_enh, y_pred_temp),\n",
    "            'f1': f1,\n",
    "            'auc': roc_auc_score(y_test_enh, y_pred_proba_ultimate)\n",
    "        }\n",
    "\n",
    "print(f\" ìµœì  ì„ê³„ê°’: {best_threshold_ultimate:.4f}\")\n",
    "print(\"ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ (í–¥ìƒëœ ë°ì´í„° + ìµœì í™”)\")\n",
    "print(f\"  Accuracy:  {best_results_ultimate['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {best_results_ultimate['precision']:.4f}\")\n",
    "print(f\"  Recall:    {best_results_ultimate['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {best_results_ultimate['f1']:.4f}\")\n",
    "print(f\"  AUC:       {best_results_ultimate['auc']:.4f}\")\n",
    "\n",
    "if best_results_ultimate['f1'] >= 0.9:\n",
    "    print(\"F1 Score 0.9 ë‹¬ì„±!\")\n",
    "elif best_results_ultimate['f1'] >= 0.8:\n",
    "    print(\" F1 Score 0.8 ì´ìƒ ë‹¬ì„±!\")\n",
    "elif best_results_ultimate['f1'] >= 0.7:\n",
    "    print(\" F1 Score 0.7 ì´ìƒ ë‹¬ì„±!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8612f9e",
   "metadata": {},
   "source": [
    "## 1ï¸1ï¸. ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7d512a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ì €ì¥ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\n",
      "ì €ì¥ ìœ„ì¹˜: ../models/2024_churn_model/\n"
     ]
    }
   ],
   "source": [
    "# ìµœì¢… ëª¨ë¸ ë° ê´€ë ¨ ê°ì²´ ì €ì¥\n",
    "import pickle\n",
    "\n",
    "print(\"ëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "with open('../models/2024_churn_model/stacking_ultimate.pkl', 'wb') as f:\n",
    "    pickle.dump(stacking_ultimate, f)\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥\n",
    "with open('../models/2024_churn_model/scaler_enh.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_enh, f)\n",
    "\n",
    "# ë”¥ëŸ¬ë‹ ëª¨ë¸ ì €ì¥\n",
    "nn_model.save('../models/2024_churn_model/nn_model.h5')\n",
    "\n",
    "# ìµœì  ì„ê³„ê°’ ì €ì¥\n",
    "with open('../models/2024_churn_model/best_threshold.txt', 'w') as f:\n",
    "    f.write(str(best_threshold_ultimate))\n",
    "\n",
    "print(\"ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(\"ì €ì¥ ìœ„ì¹˜: ../models/2024_churn_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2fb4e",
   "metadata": {},
   "source": [
    "## 1ï¸2ï¸. í•™ìŠµ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d1c18f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ í•™ìŠµ ìš”ì•½\n",
      "ì™„ë£Œëœ ì‘ì—…:\n",
      "  1. ë°ì´í„° ì „ì²˜ë¦¬ (NaN ì œê±°, ìŠ¤ì¼€ì¼ë§)\n",
      "  2. SMOTEë¥¼ í†µí•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°\n",
      "  3. 11ê°œì˜ ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„± (íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§)\n",
      "  4. 6ê°œ ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ (Baseline)\n",
      "  5. XGBoost & LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
      "  6. ë”¥ëŸ¬ë‹ ëª¨ë¸ (Advanced NN) í•™ìŠµ\n",
      "  7. Stacking Ensemble êµ¬ì¶• (4ê°œ ìµœì í™” ëª¨ë¸)\n",
      "  8. ì„ê³„ê°’ ìµœì í™” (0.005 ë‹¨ìœ„)\n",
      "  9. ìµœì¢… ëª¨ë¸ ì €ì¥\n",
      "ìµœì¢… ì„±ëŠ¥:\n",
      "  - ëª¨ë¸: Stacking Ensemble (Enhanced)\n",
      "  - F1 Score: 0.9188\n",
      "  - AUC: 0.9851\n",
      "  - ìµœì  ì„ê³„ê°’: 0.3000\n",
      " ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! Model_Evaluation.ipynbì—ì„œ ê²€ì¦ì„ ì§„í–‰í•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "print(\"ëª¨ë¸ í•™ìŠµ ìš”ì•½\")\n",
    "\n",
    "\n",
    "print(\"ì™„ë£Œëœ ì‘ì—…:\")\n",
    "print(\"  1. ë°ì´í„° ì „ì²˜ë¦¬ (NaN ì œê±°, ìŠ¤ì¼€ì¼ë§)\")\n",
    "print(\"  2. SMOTEë¥¼ í†µí•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°\")\n",
    "print(\"  3. 11ê°œì˜ ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„± (íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§)\")\n",
    "print(\"  4. 6ê°œ ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ (Baseline)\")\n",
    "print(\"  5. XGBoost & LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\")\n",
    "print(\"  6. ë”¥ëŸ¬ë‹ ëª¨ë¸ (Advanced NN) í•™ìŠµ\")\n",
    "print(\"  7. Stacking Ensemble êµ¬ì¶• (4ê°œ ìµœì í™” ëª¨ë¸)\")\n",
    "print(\"  8. ì„ê³„ê°’ ìµœì í™” (0.005 ë‹¨ìœ„)\")\n",
    "print(\"  9. ìµœì¢… ëª¨ë¸ ì €ì¥\")\n",
    "\n",
    "print(f\"ìµœì¢… ì„±ëŠ¥:\")\n",
    "print(f\"  - ëª¨ë¸: Stacking Ensemble (Enhanced)\")\n",
    "print(f\"  - F1 Score: {best_results_ultimate['f1']:.4f}\")\n",
    "print(f\"  - AUC: {best_results_ultimate['auc']:.4f}\")\n",
    "print(f\"  - ìµœì  ì„ê³„ê°’: {best_threshold_ultimate:.4f}\")\n",
    "\n",
    "print(\" ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! Model_Evaluation.ipynbì—ì„œ ê²€ì¦ì„ ì§„í–‰í•˜ì„¸ìš”.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
